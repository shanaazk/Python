{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authorized-parliament",
   "metadata": {},
   "source": [
    "## Project 1 Exercise 2 - Regression, Bike sharing Dataset\n",
    "##### <i>By Shanaaz Kamal</i>\n",
    "##### <i>Module: 212BUS-212A-1 : Analyzing Big Data II</i>\n",
    "##### I'll be using the bike sharing dataset to predict the count of casual, registered and both types of riders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-sunrise",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bronze-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dmba import regressionSummary\n",
    "from dmba import adjusted_r2_score, AIC_score, BIC_score\n",
    "from dmba import forward_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-prescription",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southern-amazon",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'day.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f14b4c0c08c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mday_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"day.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhour_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hour.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'day.csv'"
     ]
    }
   ],
   "source": [
    "day_df = pd.read_csv(\"day.csv\")\n",
    "hour_df = pd.read_csv(\"hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-mistake",
   "metadata": {},
   "source": [
    "#### Data pre-processing (Clean and Modify data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for duplicate data, null records, number of rows and number of columns.\n",
    "print(\"For the day_df dataset,\")\n",
    "duplicate_day = day_df.duplicated()\n",
    "print(\"There are\",(duplicate_day.sum()),\"number of duplicated records\")\n",
    "print(\"There are\",sum(day_df.isnull().sum()),\"number of null records\")\n",
    "print(\"There are\",day_df.shape[0],\"number of rows and\",day_df.shape[1],\"number of columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for duplicate data, null records, number of rows and number of columns.\n",
    "print(\"For the hour_df dataset,\")\n",
    "duplicate_hour= hour_df.duplicated()\n",
    "print(\"There are\",(duplicate_hour.sum()),\"number of duplicated records\")\n",
    "print(\"There are\",sum(hour_df.isnull().sum()),\"number of null records\")\n",
    "print(\"There are\",hour_df.shape[0],\"number of rows and\",hour_df.shape[1],\"number of columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for number of unique values\n",
    "day_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for number of unique values\n",
    "hour_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To view column type\n",
    "day_df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "## View the day_df information, especially for the numerical variables [temp,atemp,hum,windspeed]\n",
    "day_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "## View the hour_df information, especially for the numerical variables [temp,atemp,hum,windspeed]\n",
    "hour_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical variables for hour_df dataset\n",
    "categorical_vars_hour = ['yr','holiday','workingday','season','mnth','hr','weekday','weathersit']\n",
    "\n",
    "## Categorical variables for day_df dataset\n",
    "categorical_vars = ['season','yr','mnth','holiday','weekday','workingday','weathersit']\n",
    "\n",
    "## Store the numerical variable into a list\n",
    "numerical_vars =['temp','atemp','hum','windspeed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-neighbor",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the categorical variables\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(50,30))\n",
    "\n",
    "## Reduce the space between the plots\n",
    "gs = fig.add_gridspec(5,5)\n",
    "gs.update(wspace=0.2, hspace=0.15)\n",
    "fig = plt.figure(figsize=(50,30))\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "ax1 = fig.add_subplot(gs[0,1])\n",
    "ax2 = fig.add_subplot(gs[0,2])\n",
    "ax3 = fig.add_subplot(gs[1,0])\n",
    "ax4 = fig.add_subplot(gs[1,1])\n",
    "ax5 = fig.add_subplot(gs[1,2])\n",
    "ax6 = fig.add_subplot(gs[2,0])\n",
    "ax7 = fig.add_subplot(gs[2,1])\n",
    "\n",
    "color_palette = \"magma\"\n",
    "\n",
    "ax0.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax0,data=day_df,x='season',palette=color_palette)\n",
    "ax0.set_xlabel(\"Season\",fontsize=18)\n",
    "ax0.set_ylabel(\"count\",fontsize=18)\n",
    "\n",
    "ax1.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax1,data=day_df,x='mnth',palette=color_palette)\n",
    "ax1.set_xlabel(\"Month\",fontsize=18)\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "ax2.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax2,data=day_df,x='weekday',palette=color_palette)\n",
    "ax2.set_xlabel(\"Day\", fontsize=18)\n",
    "ax2.set_ylabel(\"count\", fontsize=18)\n",
    "\n",
    "ax3.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax3,data=day_df,x='weathersit',palette=color_palette)\n",
    "ax3.set_xlabel(\"Weather Situation\", fontsize=18)\n",
    "ax3.set_ylabel(\"\")\n",
    "\n",
    "ax4.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax4,data=hour_df,x='hr',palette=color_palette)\n",
    "ax4.set_xlabel(\"Hour\", fontsize=18)\n",
    "ax4.set_ylabel(\"\")\n",
    "\n",
    "ax5.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax5,data=day_df,x='yr',palette=color_palette)\n",
    "ax5.set_xlabel(\"Year\", fontsize=18)\n",
    "ax5.set_ylabel(\"\")\n",
    "\n",
    "ax6.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax6,data=day_df,x='holiday',palette=color_palette)\n",
    "ax6.set_xlabel(\"Holiday\", fontsize=18)\n",
    "ax6.set_ylabel(\"\")\n",
    "\n",
    "ax7.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.countplot(ax=ax7,data=day_df,x='workingday',palette=color_palette)\n",
    "ax7.set_xlabel(\"Working Day\", fontsize=18)\n",
    "ax7.set_ylabel(\"\")\n",
    "\n",
    "\n",
    "ax0.grid(True, which='major')\n",
    "ax1.grid(True, which='major')\n",
    "ax2.grid(True, which='major')\n",
    "ax3.grid(True, which='major')\n",
    "ax4.grid(True, which='major')\n",
    "ax5.grid(True, which='major')\n",
    "ax6.grid(True, which='major')\n",
    "ax7.grid(True, which='major')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the numerical/continuous variables (not including the indicator/dummy variables)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(40,30))\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "ax1 = fig.add_subplot(gs[0,1])\n",
    "ax2 = fig.add_subplot(gs[1,0])\n",
    "ax3 = fig.add_subplot(gs[1,1])\n",
    "\n",
    "## Reduce the space between the plots\n",
    "gs = fig.add_gridspec(4,4)\n",
    "gs.update(wspace=0.2, hspace=0.15)\n",
    "\n",
    "color_palette = \"magma\"\n",
    "\n",
    "ax0.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.boxenplot(ax=ax0,data=day_df,y='temp',palette=color_palette, width=0.3)\n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.set_ylabel(\"Normalized Temperature\",fontsize=18)\n",
    "\n",
    "ax1.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.boxenplot(ax=ax1,data=day_df,y='atemp',palette=color_palette, width=0.3)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"Feeling Temperature\",fontsize=18)\n",
    "\n",
    "ax2.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.boxenplot(ax=ax2,data=day_df,y='hum',palette=color_palette, width=0.3)\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylabel(\"Humidity\",fontsize=18)\n",
    "\n",
    "ax3.grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "sns.boxenplot(ax=ax3,data=day_df,y='windspeed',palette=color_palette, width=0.3)\n",
    "ax3.set_xlabel(\"\")\n",
    "ax3.set_ylabel(\"Wind Speed\",fontsize=18)\n",
    "\n",
    "\n",
    "ax0.grid(True, which='major')\n",
    "ax1.grid(True, which='major')\n",
    "ax2.grid(True, which='major')\n",
    "ax3.grid(True, which='major')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the correlation matrix between the variables in day_df dataset\n",
    "corr = day_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the correlation matrix between the variables in hour_df dataset\n",
    "corr = hour_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize relationship between the target variables ('casual','registered',cnt) and predictors/features for day_df\n",
    "y_vars = [\"casual\",\"registered\",\"cnt\"]\n",
    "sns.pairplot(day_df,hue='yr',palette = [\"#8000ff\",\"#da8829\"],y_vars=y_vars,height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize relationship between the target variables ('casual','registered',cnt) and predictors/features for hour_df\n",
    "y_vars = [\"casual\",\"registered\",\"cnt\"]\n",
    "sns.pairplot(hour_df,hue='yr',palette = [\"#8000ff\",\"#da8829\"],y_vars=y_vars,height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the relationship between the numerical variables and target variables in day_df dataset\n",
    "\n",
    "x_vars = [\"temp\",\"atemp\",\"hum\",\"windspeed\"]\n",
    "y_vars = [\"casual\",\"registered\",\"cnt\"]\n",
    "g = sns.PairGrid(day_df, hue=\"yr\", x_vars=x_vars, y_vars=y_vars,palette=\"magma\",height=6,aspect=0.9)\n",
    "g.map_diag(sns.histplot) \n",
    "g.map_offdiag(sns.scatterplot)\n",
    "g.add_legend(title='Year', fontsize= '12')\n",
    "plt.figure(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the relationship between the numerical variables and target variables in hour_df\n",
    " \n",
    "x_vars = [\"temp\",\"atemp\",\"hum\",\"windspeed\"]\n",
    "y_vars = [\"casual\",\"registered\",\"cnt\"]\n",
    "g = sns.PairGrid(hour_df, hue=\"yr\", x_vars=x_vars, y_vars=y_vars,palette=\"magma\",height=6,aspect=0.9)\n",
    "g.map_diag(sns.histplot) \n",
    "g.map_offdiag(sns.scatterplot)\n",
    "g.add_legend(title='Year', fontsize= '12')\n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.set_ylabel(\"Normalized Temperature\",fontsize=18)\n",
    "\n",
    "plt.figure(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-candle",
   "metadata": {},
   "source": [
    "### Data Observations ###\n",
    "* No duplicates/null in the data\n",
    "* There more number of riders when the weather situation = 1(Clear, Few Clouds, Partly Cloudy)\n",
    "* There are more registered riders in 2012 than 2011\n",
    "* Registered riders use the bike when temperatures, humidity and windspeed are more optimal, in comparison to casual riders\n",
    "* Casual riders do not use the bike when windspeed is high\n",
    "* Based on the correlation matrix, there is not much collinearity among the variables in both the day and hour datasets\n",
    "* From the pairgrid, we can tell that there are more casual/registered/both riders for higher humidity\n",
    "* Based on the pairplot, there seems to be better distribution of the data for the day_df dataset\n",
    "\n",
    "##### I will proceed in using the day dataset to predict the target variables casual, registered and both type users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-shooting",
   "metadata": {},
   "source": [
    "#### Split the dataset before scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(day_df, test_size=0.2)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-lucas",
   "metadata": {},
   "source": [
    "#### Scale and Transform the dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialized the MinMaxScaler to scale the numericals variabls to 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "train_transformed_df=pd.DataFrame(scaler.fit_transform(train_df[numerical_vars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Added the column names\n",
    "train_transformed_df.columns=['temp','atemp','hum','windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verified shape\n",
    "train_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain the dummy variables for the categorical data\n",
    "def get_one_hot_encoded_col(train_df, test_df, input_col):\n",
    "    enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "    enc.fit(np.array(train_df[input_col]).reshape(-1, 1))\n",
    "\n",
    "    encoded_train = enc.transform(np.array(train_df[input_col]).reshape(-1, 1)).toarray()\n",
    "    encoded_test = enc.transform(np.array(test_df[input_col]).reshape(-1, 1)).toarray()\n",
    "    \n",
    "    encoded_cols = ['{}_{}'.format(input_col, i) for i in range(encoded_train.shape[1])]\n",
    "    \n",
    "    return encoded_train, encoded_test, encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_season_train, oh_season_test, oh_season_cols = get_one_hot_encoded_col(train_df, test_df, 'season')\n",
    "oh_yr_train, oh_yr_test, oh_yr_cols = get_one_hot_encoded_col(train_df, test_df, 'yr')\n",
    "oh_mnth_train, oh_mnth_test, oh_mnth_cols = get_one_hot_encoded_col(train_df, test_df, 'mnth')\n",
    "oh_holiday_train, oh_holiday_test, oh_holiday_cols = get_one_hot_encoded_col(train_df, test_df, 'holiday')\n",
    "oh_weekday_train, oh_weekday_test, oh_weekday_cols = get_one_hot_encoded_col(train_df, test_df, 'weekday')\n",
    "oh_workdingday_train, oh_workdingday_test, oh_workingday_cols = get_one_hot_encoded_col(train_df, test_df, 'workingday')\n",
    "oh_weathersit_train, oh_weathersit_test, oh_weathersit_cols = get_one_hot_encoded_col(train_df, test_df, 'weathersit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_categorical_train_df = pd.DataFrame(np.concatenate((oh_season_train, oh_yr_train, \n",
    "                                                    oh_mnth_train, oh_holiday_train, oh_weekday_train, oh_workdingday_train, \n",
    "                                                    oh_weathersit_train), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_df = pd.concat([train_transformed_df,dummy_categorical_train_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add columns to the converted dataframe\n",
    "train_transformed_df.columns = ['temp', 'atemp', 'hum', 'windspeed'] + oh_season_cols + oh_yr_cols + oh_mnth_cols + oh_holiday_cols + oh_weekday_cols + oh_workingday_cols + oh_weathersit_cols\n",
    "\n",
    "## Print column names\n",
    "train_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the test dataset\n",
    "test_transformed_df=pd.DataFrame(scaler.transform(test_df[numerical_vars]))\n",
    "test_transformed_df.columns=['temp','atemp','hum','windspeed']\n",
    "test_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_categorical_test_df = pd.DataFrame(np.concatenate((oh_season_test, oh_yr_test, \n",
    "                                                    oh_mnth_test, oh_holiday_test, oh_weekday_test, oh_workdingday_test, \n",
    "                                                    oh_weathersit_test), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed_df = pd.concat([test_transformed_df,dummy_categorical_test_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add columns to the converted dataframe\n",
    "test_transformed_df.columns = ['temp', 'atemp', 'hum', 'windspeed'] + oh_season_cols + oh_yr_cols + oh_mnth_cols + oh_holiday_cols + oh_weekday_cols + oh_workingday_cols + oh_weathersit_cols\n",
    "\n",
    "## Print column names\n",
    "test_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-green",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize linear regession model to predict count of casual riders\n",
    "casual_lr = LinearRegression()\n",
    "\n",
    "## Fit the training data to the LinearRegression model\n",
    "casual_lr.fit(train_transformed_df, train_df['casual'])\n",
    "\n",
    "casual_lr.score(train_transformed_df, train_df['casual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize linear regession model\n",
    "registered_lr = LinearRegression()\n",
    "\n",
    "## Fit the training data to the LinearRegression model\n",
    "registered_lr.fit(train_transformed_df, train_df['registered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize linear regession model\n",
    "cnt_lr = LinearRegression()\n",
    "\n",
    "## Fit the training data to the LinearRegression model\n",
    "cnt_lr.fit(train_transformed_df, train_df['cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-winter",
   "metadata": {},
   "source": [
    "#### Test the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_casual = casual_lr.predict(test_transformed_df)\n",
    "regressionSummary(test_df['casual'],test_predictions_casual)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['casual'], test_predictions_casual, casual_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['casual'], test_predictions_casual, casual_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['casual'], test_predictions_casual, casual_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_registered = registered_lr.predict(test_transformed_df)\n",
    "regressionSummary(test_df['registered'],test_predictions_registered)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['registered'], test_predictions_registered, registered_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['registered'], test_predictions_registered, registered_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['registered'], test_predictions_registered, registered_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_both = cnt_lr.predict(test_transformed_df)\n",
    "regressionSummary(test_df['cnt'],test_predictions_both)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['cnt'], test_predictions_both, cnt_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['cnt'], test_predictions_both, cnt_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['cnt'], test_predictions_both, cnt_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-april",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-manual",
   "metadata": {},
   "source": [
    "##### <b>Forward selection for target variable, count of casual riders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward selection\n",
    "def train_model(variables):\n",
    "    if len(variables) == 0:\n",
    "        return None\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_transformed_df[variables], train_df['casual'])\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    if len(variables) == 0:\n",
    "        return AIC_score( train_df['casual'], [ train_df['casual'].mean()] * len( train_df['casual']), model, df=1)\n",
    "    return AIC_score( train_df['casual'], model.predict(train_transformed_df[variables]), model)\n",
    "\n",
    "best_model_casual, best_variables_casual = forward_selection(train_transformed_df.columns, train_model, score_model, verbose=True)\n",
    "print(best_variables_casual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-glucose",
   "metadata": {},
   "source": [
    "##### <b>Forward selection for target variable, count of registered riders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward selection\n",
    "def train_model(variables):\n",
    "    if len(variables) == 0:\n",
    "        return None\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_transformed_df[variables], train_df['registered'])\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    if len(variables) == 0:\n",
    "        return AIC_score( train_df['registered'], [ train_df['registered'].mean()] * len( train_df['registered']), model, df=1)\n",
    "    return AIC_score( train_df['registered'], model.predict(train_transformed_df[variables]), model)\n",
    "\n",
    "best_model_registered, best_variables_registered = forward_selection(train_transformed_df.columns, train_model, score_model, verbose=True)\n",
    "print(best_variables_registered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-disposal",
   "metadata": {},
   "source": [
    "##### <b>Forward selection for target variable, count of both casual and registered riders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward selection\n",
    "def train_model(variables):\n",
    "    if len(variables) == 0:\n",
    "        return None\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_transformed_df[variables], train_df['cnt'])\n",
    "    return model\n",
    "\n",
    "def score_model(model, variables):\n",
    "    if len(variables) == 0:\n",
    "        return AIC_score( train_df['cnt'], [ train_df['cnt'].mean()] * len( train_df['cnt']), model, df=1)\n",
    "    return AIC_score( train_df['cnt'], model.predict(train_transformed_df[variables]), model)\n",
    "\n",
    "best_model_cnt, best_variables_cnt = forward_selection(train_transformed_df.columns, train_model, score_model, verbose=True)\n",
    "print(best_variables_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-samba",
   "metadata": {},
   "source": [
    "#### Performance of models for casual, registered and both types of riders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify the performance of the test data using the best model and variables\n",
    "print(\"Performance of Casual riders model\")\n",
    "print(\"\\nBefore Feature Selection\")\n",
    "regressionSummary(test_df['casual'],test_predictions_casual)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['casual'], test_predictions_casual, casual_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['casual'], test_predictions_casual, casual_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['casual'], test_predictions_casual, casual_lr))\n",
    "print(\"\\nAfter Feature Selection\")\n",
    "regressionSummary(test_df['casual'], best_model_casual.predict(test_transformed_df[best_variables_casual]))\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['casual'], best_model_casual.predict(test_transformed_df[best_variables_casual]), best_model_casual))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['casual'], best_model_casual.predict(test_transformed_df[best_variables_casual]), best_model_casual))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['casual'], best_model_casual.predict(test_transformed_df[best_variables_casual]), best_model_casual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify the performance of the test data using the best model and variables\n",
    "print(\"Performance of Registered riders model\")\n",
    "print(\"\\nBefore Feature Selection\")\n",
    "regressionSummary(test_df['registered'],test_predictions_registered)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['registered'], test_predictions_registered, registered_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['registered'], test_predictions_registered, registered_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['registered'], test_predictions_registered, registered_lr))\n",
    "print(\"\\nAfter Feature Selection\")\n",
    "regressionSummary(test_df['registered'], best_model_registered.predict(test_transformed_df[best_variables_registered]))\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['registered'], best_model_registered.predict(test_transformed_df[best_variables_registered]), best_model_registered))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['registered'], best_model_registered.predict(test_transformed_df[best_variables_registered]), best_model_registered))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['registered'], best_model_registered.predict(test_transformed_df[best_variables_registered]), best_model_registered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify the performance of the test data using the best model and variables\n",
    "print(\"Performance of both Casual and Registered riders model\")\n",
    "print(\"\\nBefore Feature Selection\")\n",
    "regressionSummary(test_df['cnt'],test_predictions_both)\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['cnt'], test_predictions_both, cnt_lr))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['cnt'], test_predictions_both, cnt_lr))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['cnt'], test_predictions_both, cnt_lr))\n",
    "print(\"\\nAfter Feature Selection\")\n",
    "regressionSummary(test_df['cnt'], best_model_cnt.predict(test_transformed_df[best_variables_cnt]))\n",
    "print('\\n\\t\\tAdjusted R2 : ', adjusted_r2_score(test_df['cnt'], best_model_cnt.predict(test_transformed_df[best_variables_cnt]), best_model_cnt))\n",
    "print('\\t\\tAIC : ', AIC_score(test_df['cnt'], best_model_cnt.predict(test_transformed_df[best_variables_cnt]), best_model_cnt))\n",
    "print('\\t\\tBIC : ', BIC_score(test_df['cnt'], best_model_cnt.predict(test_transformed_df[best_variables_cnt]), best_model_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-racing",
   "metadata": {},
   "source": [
    "#### Data Observations\n",
    "* In general, the models to predict the features (casual, registered and both) performed better after feature selection\n",
    "* This is substantiated by the higher adjusted R-squared values and the lower RMSE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
